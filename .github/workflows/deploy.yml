# .github/workflows/deploy.yml

name: Deploy Django Financial App to EC2

# Controls when the workflow will run
on:
  push:
    branches: [ master ] # Trigger on pushes to the master branch

jobs:
  deploy:
    runs-on: ubuntu-latest # Use the latest Ubuntu runner provided by GitHub

    steps:
    # Step 1: Checkout the repository code
    - name: Checkout code
      uses: actions/checkout@v4

    # Step 2: Set up Python environment
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12' # Match the Python version on your EC2 instance

    # Step 3: Install Python dependencies
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    # Step 4: Run collectstatic
    - name: Run collectstatic
      env: # Make secrets and necessary env vars available for this step
        # AWS Credentials for django-storages (if not using instance profile for collectstatic)
        # These are needed if your settings.py tries to connect to S3 during import/setup
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_STORAGE_BUCKET_NAME: ${{ secrets.AWS_STORAGE_BUCKET_NAME }}
        AWS_S3_REGION_NAME: ${{ secrets.AWS_S3_REGION_NAME }} # Ensure this matches your bucket region
        CLOUDFRONT_DOMAIN: ${{ secrets.CLOUDFRONT_DOMAIN }}
        # Required by settings.py to avoid raising an error
        SECRET_KEY: "dummy-secret-key-for-collectstatic" # A non-empty dummy key is usually fine for collectstatic
        DEBUG: "False" # Ensure DEBUG is False for collectstatic
        # Add any other environment variables your settings.py might expect to be present
        # even if they are not directly used by collectstatic, to prevent load-time errors.
        # For example, if database settings are accessed at import time:
        # DB_ENGINE: "django.db.backends.postgresql" # Dummy value if not actually connecting
        # DB_NAME: "dummy_db"
        # etc.
        # CELERY_SQS_QUEUE_URL: "dummy_sqs_url_for_collectstatic" # If settings.py tries to read it
      run: |
        echo "Running collectstatic..."
        python manage.py collectstatic --noinput
        echo "Collectstatic finished."

    # Step 5: Configure AWS credentials (for subsequent AWS CLI commands like SSM)
    - name: Configure AWS Credentials for Deployment
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    # Step 6: Sync static files to S3 (This step is now primarily handled by collectstatic with django-storages)
    # The collectstatic step above, when using S3Boto3Storage, should directly upload to S3.
    # No separate sync step is typically needed if STATICFILES_STORAGE is correctly configured.

    # Step 7: Deploy to EC2 using AWS SSM Run Command
    - name: Deploy to EC2
      run: |
        # Define commands as a multi-line string (heredoc)
        COMMANDS_HEREDOC=$(cat <<EOF
        cd /srv/financial-app
        git fetch origin master
        git reset --hard origin/master
        source venv/bin/activate
        pip install -r requirements.txt
        eval \$(python3 load_params.py)
        python3 manage.py migrate --noinput
        sudo systemctl restart gunicorn
        sudo systemctl restart celery-worker
        echo "Deployment completed successfully on EC2"
        EOF
        )

        # Prepare commands for JSON array format
        JSON_PARAMS_PAYLOAD="{\"commands\":["
        FIRST_CMD=true
        while IFS= read -r cmd; do
          if [ "$FIRST_CMD" = true ]; then
            FIRST_CMD=false
          else
            JSON_PARAMS_PAYLOAD="$JSON_PARAMS_PAYLOAD,"
          fi
          ESCAPED_CMD=$(echo "$cmd" | sed 's/\\/\\\\/g' | sed 's/"/\\"/g')
          JSON_PARAMS_PAYLOAD="$JSON_PARAMS_PAYLOAD\"$ESCAPED_CMD\""
        done <<< "$COMMANDS_HEREDOC"
        JSON_PARAMS_PAYLOAD="$JSON_PARAMS_PAYLOAD]}"
        
        echo "Final JSON for SSM: $JSON_PARAMS_PAYLOAD" # For debugging

        # Send commands to EC2 instance via SSM Run Command
        aws ssm send-command \
          --instance-ids ${{ secrets.EC2_INSTANCE_ID }} \
          --document-name "AWS-RunShellScript" \
          --parameters "$JSON_PARAMS_PAYLOAD" \
          --comment "Deploying new version of Financial App via GitHub Actions" \
          --output text

    # Optional: Add a step to check command status if needed
    # - name: Check SSM Command Status
    #   run: |
    #     # Add logic to wait and check the status of the send-command invocation
    #     echo "Deployment script sent. Check SSM console for execution status."
